{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM 中特征重要性的计算方法及其思想\n",
    "\n",
    "在 **LightGBM**（Light Gradient Boosting Machine）中，特征重要性（Feature Importance）是评估各个输入特征对模型预测贡献大小的关键指标。LightGBM 提供了多种计算特征重要性的方法，每种方法基于不同的思想和计算方式。理解这些方法有助于深入了解模型的决策过程，优化特征工程，并提升模型性能。\n",
    "\n",
    "以下是 LightGBM 中常用的几种特征重要性计算方式及其背后的思想："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 基于分裂次数的特征重要性（Split Importance）**\n",
    "\n",
    "### **1.1 概述**\n",
    "\n",
    "**Split Importance** 是最直观和常用的特征重要性计算方法。它通过统计一个特征在所有树中被用作分裂节点的次数来衡量该特征的重要性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 计算方式**\n",
    "\n",
    "- **分裂次数**：统计每个特征在所有树中作为分裂节点出现的总次数。\n",
    "- **归一化**：将每个特征的分裂次数除以所有特征分裂次数的总和，得到相对重要性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 思想**\n",
    "\n",
    "这个方法的核心思想是，频繁用于分裂的特征对模型的预测贡献更大，因为它们在多个决策树中被反复使用来区分不同的样本。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.4 优缺点**\n",
    "\n",
    "- **优点**：\n",
    "  - 简单直观，易于理解和实现。\n",
    "  - 计算效率高，适用于大规模数据集。\n",
    "  \n",
    "- **缺点**：\n",
    "  - 不考虑特征分裂对模型性能的实际贡献。\n",
    "  - 可能偏向于高基数（high cardinality）的特征，即具有更多唯一值的特征更容易被选择为分裂节点。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 基于信息增益的特征重要性（Gain Importance）**\n",
    "\n",
    "### **2.1 概述**\n",
    "\n",
    "**Gain Importance** 衡量每个特征在所有树中分裂节点所带来的目标函数（如损失函数）的改善总和。换句话说，它计算了使用该特征进行分裂所带来的信息增益。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 计算方式**\n",
    "\n",
    "- **信息增益**：对于每个分裂节点，计算该分裂带来的目标函数改善量。\n",
    "- **累加**：将所有分裂节点的信息增益累加到对应的特征上。\n",
    "- **归一化**：将每个特征的总信息增益除以所有特征信息增益的总和，得到相对重要性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 思想**\n",
    "\n",
    "这个方法不仅考虑了特征被使用的次数，还考虑了每次使用时对模型性能的具体贡献。一个在关键节点上带来大量信息增益的特征，其重要性会被更高地评估。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4 优缺点**\n",
    "\n",
    "- **优点**：\n",
    "  - 更能反映特征对模型性能的实际贡献。\n",
    "  - 不易偏向高基数特征。\n",
    "  \n",
    "- **缺点**：\n",
    "  - 计算相对复杂，尤其是在树的深度较大时。\n",
    "  - 可能对噪声特征敏感，若噪声特征偶尔带来高信息增益，可能被错误评估为重要特征。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. 基于覆盖度的特征重要性（Cover Importance）**\n",
    "\n",
    "### **3.1 概述**\n",
    "\n",
    "**Cover Importance** 通过统计每个特征在分裂节点上所覆盖的样本数量来衡量特征的重要性。它反映了特征在模型中影响的样本范围。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 计算方式**\n",
    "\n",
    "- **样本覆盖**：对于每个分裂节点，计算被该节点覆盖的样本数量。\n",
    "- **累加**：将所有分裂节点的覆盖样本数量累加到对应的特征上。\n",
    "- **归一化**：将每个特征的总覆盖样本数量除以所有特征覆盖样本数量的总和，得到相对重要性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3 思想**\n",
    "\n",
    "该方法侧重于衡量特征在模型中影响的广度，即一个特征在多少样本中起到了分裂作用。覆盖度高的特征通常对大多数样本具有重要影响。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4 优缺点**\n",
    "\n",
    "- **优点**：\n",
    "  - 直观反映特征影响的样本范围。\n",
    "  - 适用于需要了解特征影响广度的场景。\n",
    "  \n",
    "- **缺点**：\n",
    "  - 不直接反映特征对模型性能的贡献。\n",
    "  - 可能高估那些分裂样本较多但信息增益较低的特征。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. 基于 SHAP 值的特征重要性（SHAP Importance）**\n",
    "\n",
    "### **4.1 概述**\n",
    "\n",
    "**SHAP（SHapley Additive exPlanations）** 是一种基于合作博弈论的特征重要性解释方法。它为每个特征分配一个 SHAP 值，表示该特征对模型预测的贡献大小。SHAP 值不仅提供了全局特征重要性，还可以进行局部解释。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 计算方式**\n",
    "\n",
    "- **Shapley 值**：计算每个特征在不同特征子集下对预测结果的边际贡献。\n",
    "- **汇总**：对所有实例和所有特征的 SHAP 值进行平均，得到全局特征重要性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![本地图片](./图片/shap.png \"shap value的计算\"){ width=50% }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3 思想**\n",
    "\n",
    "SHAP 值结合了基于分裂次数和基于信息增益的优点，通过考虑特征在不同组合下的贡献，提供了更为全面和公平的特征重要性评估。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4 优缺点**\n",
    "\n",
    "- **优点**：\n",
    "  - 提供更精确和一致的特征重要性解释。\n",
    "  - 结合了局部和全局解释，适用于多种场景。\n",
    "  \n",
    "- **缺点**：\n",
    "  - 计算成本较高，尤其是对于大规模数据集和复杂模型。\n",
    "  - 需要额外的计算资源和时间。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.5 应用场景**\n",
    "\n",
    "- **模型调试与优化**：识别对模型预测贡献最大的特征，指导特征工程。\n",
    "- **业务解释与决策支持**：向非技术利益相关者解释模型决策过程，提升模型的透明度。\n",
    "- **合规与透明性**：满足行业法规对模型可解释性的要求。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. 基于 Permutation 的特征重要性（Permutation Importance）**\n",
    "\n",
    "### **5.1 概述**\n",
    "\n",
    "**Permutation Importance** 是一种模型无关的特征重要性计算方法。它通过随机打乱一个特征的值，观察模型性能的下降程度来衡量该特征的重要性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2 计算方式**\n",
    "\n",
    "1. **基准性能评估**：首先在未打乱特征的测试集上评估模型的性能指标（如 AUC、RMSE）。\n",
    "2. **特征打乱**：依次对每个特征进行打乱（随机重排）。\n",
    "3. **重新评估性能**：在打乱后的测试集上评估模型的性能指标。\n",
    "4. **计算重要性**：特征打乱后性能下降的幅度即为该特征的重要性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.3 思想**\n",
    "\n",
    "Permutation Importance 通过量化特征对模型性能的影响，提供了一种直观且模型无关的特征重要性评估方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.4 优缺点**\n",
    "\n",
    "- **优点**：\n",
    "  - 模型无关，适用于任何类型的模型。\n",
    "  - 直接反映特征对模型性能的实际贡献。\n",
    "  \n",
    "- **缺点**：\n",
    "  - 计算成本高，尤其是特征数量多或模型复杂时。\n",
    "  - 对于高度相关的特征，可能会相互影响，导致重要性评估不准确。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. 基于 Gain 和 Split 的综合特征重要性（Combined Importance）**\n",
    "\n",
    "### **6.1 概述**\n",
    "\n",
    "在某些情况下，可以结合 **Gain** 和 **Split** 两种方法，综合评估特征的重要性。这种方法考虑了特征的使用频率和对模型性能的具体贡献，提供了更全面的特征重要性视角。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.2 计算方式**\n",
    "\n",
    "- **综合权重**：为每个特征分配基于 Gain 和 Split 的权重，例如通过加权平均或其他组合策略。\n",
    "- **归一化**：将综合权重进行归一化，得到相对重要性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.3 思想**\n",
    "\n",
    "通过结合 Gain 和 Split 的优势，综合特征的使用频率和实际贡献，避免单一方法可能带来的偏差，提升特征重要性评估的准确性和可靠性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.4 优缺点**\n",
    "\n",
    "- **优点**：\n",
    "  - 提供更全面的特征重要性评估。\n",
    "  - 结合多种信息源，提升评估的准确性。\n",
    "  \n",
    "- **缺点**：\n",
    "  - 增加了计算复杂度。\n",
    "  - 需要设计合理的组合策略，以避免权重失衡。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. 总结**\n",
    "\n",
    "LightGBM 提供了多种特征重要性计算方法，每种方法基于不同的思想和侧重点：\n",
    "\n",
    "1. **Split Importance**：基于特征在分裂节点上的使用频率，简单直观但可能偏向高基数特征。\n",
    "2. **Gain Importance**：基于特征分裂节点带来的信息增益，反映特征对模型性能的实际贡献。\n",
    "3. **Cover Importance**：基于特征分裂节点覆盖的样本数量，反映特征影响的广度。\n",
    "4. **SHAP Importance**：基于 Shapley 值，提供更精确和一致的特征重要性解释，适用于多种场景。\n",
    "5. **Permutation Importance**：模型无关，通过特征打乱评估对模型性能的影响，适用于任何模型类型。\n",
    "6. **Combined Importance**：结合 Gain 和 Split 的优势，提供更全面的特征重要性评估。\n",
    "\n",
    "### **最佳实践**\n",
    "\n",
    "- **多方法结合**：结合多种特征重要性计算方法，综合评估特征的影响，避免单一方法的偏差。\n",
    "- **上下文理解**：根据具体应用场景和业务需求，选择最合适的特征重要性计算方法。\n",
    "- **验证与解释**：结合可视化工具（如 SHAP 的 summary_plot、force_plot），深入理解特征对模型预测的具体影响。\n",
    "- **特征选择与工程**：根据特征重要性结果，进行特征选择、特征构造或特征删除，以优化模型性能和泛化能力。\n",
    "\n",
    "通过合理应用这些特征重要性计算方法，可以更好地理解和优化 LightGBM 模型，提升模型的预测能力和解释性。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
